{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc26c543-3aae-4716-bcef-fa8974f808f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Autodiff Tutorial\n",
    "\n",
    "In this tutorial we'll be implementing a minimal autodiff library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8883f-c557-45a6-ae9f-35d97e880c0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Credits, reading/watching material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce43b14-62aa-4a20-b921-6329c1474381",
   "metadata": {},
   "source": [
    "\n",
    "It is heavily based on the [autodiff](https://mdrk.io/introduction-to-automatic-differentiation/) [tutorials](https://mdrk.io/introduction-to-automatic-differentiation-part2/) by [Marcus D. R. Klarqvist](https://mdrk.io/). I strongly encourage you to carefully read those tutorials, which include deeper explanations.\n",
    "\n",
    "Other resources I studied to make this tutorial\n",
    "* [micrograd](https://github.com/karpathy/micrograd) by [Andrej Karpathy](https://karpathy.ai/)\n",
    "* https://douglasorr.github.io/2021-11-autodiff/article.html\n",
    "* https://sidsite.com/posts/autodiff/\n",
    "* [video series](https://www.youtube.com/watch?v=RxmBukb-Om4&list=PLeDtc0GP5ICldMkRg-DkhpFX1rRBNHTCs) by [Joel Grus](https://joelgrus.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc3673-1081-47b6-a30b-faf8712dd0d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Differentiation rules (a refresher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c0753-8af9-42cd-80af-84aa1164a4f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Let's remember some of the rules of differentiation:\n",
    "\n",
    "## Linear rules ([differentiation is linear](https://en.wikipedia.org/wiki/Linearity_of_differentiation))\n",
    "\n",
    "For any functions $f$ and $g$ and any real numbers $a$ and $b$, the derivative of the function $h(x) = af(x) + bg(x)$ w.r.t $x$ is:\n",
    "\n",
    "$h'(x) = af'(x) + bg'|(x)$\n",
    "\n",
    "some special cases are:\n",
    "\n",
    "* constant factor $h(x) = af(x)$\n",
    "  \n",
    "  $h'(x) = af'(x)$\n",
    "\n",
    "* Sum $h(x) = f(x) + g(x)$\n",
    "\n",
    "  $h'(x) = f'(x) + g'(x)$\n",
    "\n",
    "* Subtraction $h(x) = f(x) - g(x)$\n",
    "\n",
    "  $h'(x) = f'(x) - g'(x)$\n",
    "\n",
    "### Product rule\n",
    "\n",
    "If $h(x) = f(x)g(x)$:\n",
    "\n",
    "$h'(x) = f'(x)g(x) + f(x)g'(x)$\n",
    "\n",
    "### Quotient rule\n",
    "\n",
    "If $\\displaystyle h(x) = \\frac{f(x)}{g(x)}$:\n",
    "\n",
    "$\\displaystyle h'(x) = \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}$\n",
    "\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "The main player of the autodifferentiation idea! If $h(x) = f(g(x))$:\n",
    "\n",
    "$h'(x) = f'(g(x)) \\cdot f(x)g'(x)$\n",
    "\n",
    "A common variable replacement and using [Leibniz's notation](https://en.wikipedia.org/wiki/Leibniz%27s_notation). if $y = f(u)$ and $u = g(x)$:\n",
    "\n",
    "$\\displaystyle\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}y}{\\mathrm{d}u}\\cdot\\frac{\\mathrm{d}u}{\\mathrm{d}x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5afc4-bbcd-4bc7-b584-bf76d4fb0b92",
   "metadata": {},
   "source": [
    "# Implementing Automatic differentiation\n",
    "\n",
    "Using the chain rule, we can build a system that automatically converts a function into a sequence of operations for which we know the derivative.\n",
    "\n",
    "Modern autodiff can be implemented in two ways:\n",
    "* **forward-mode:** The derivatives are calculated in a single pass through the computation graph.\n",
    "* **reverse-mode:** The derivatives are calculated starting from the output, and traversing the computation graph in reverse order.\n",
    "\n",
    "> Throughout this tutorial, we will be implementing a `Variable` class using [operator overloading](https://en.wikipedia.org/wiki/Operator_overloading), which in python requires implementing [special names](https://docs.python.org/3/reference/datamodel.html#special-method-names) for some of the basic arithmetic operators. \n",
    "\n",
    "In general, once you fully grasp the idea of function composition and the chain rule, you will realize that we can effectively implement a differentiation engine with basically 2 lines of code per operation we want to support!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832aa1fd-c15a-4d77-a51e-da4e0d5d5c6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Forward-mode autodiff\n",
    "\n",
    "While the end goal is to implement reverse-mode, we must understand the forward-mode first. Maybe the best way to understand it is to implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e166af2-98ab-4354-91cd-2924fef6685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c74f2-c22a-4053-acb7-37117b473b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable():\n",
    "    def __init__(self, f, d = 0):\n",
    "        self.f = f # the function evaluation: f(x)\n",
    "        self.d = d # the derivative: f'(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # this function allows us to see the values of the class members when printing\n",
    "        return f\"Var(f={self.f}, d={self.d})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19430749-60e3-421f-9d28-ef6742b519e8",
   "metadata": {},
   "source": [
    "let's test the creation of a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3d33e-4ea5-4c3c-a211-98a192f78e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(1, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b3ba3-3a85-454c-bffb-d8c6620370ac",
   "metadata": {},
   "source": [
    "Let's overload our first arithmetic operator. Where possible, I will use [lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions), which allows us to define anonymous functions with less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4677e33-48d4-4357-8999-53e0b114fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "Variable.__add__ = lambda self, other: Variable(self.f + other.f, self.d + other.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5758a-6d9e-4014-b7c5-afa0be2ca1a0",
   "metadata": {},
   "source": [
    "We can test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b93e07-e353-4303-bf63-7f3c8b1db74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(1, 2)\n",
    "b = Variable(3, 4)\n",
    "print(f\"    a = {a}\")\n",
    "print(f\"    b = {b}\")\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"b + a = {b + a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc1659-94ad-401c-841b-f1103e03b62b",
   "metadata": {},
   "source": [
    "And now we can add (and test), the remaining basic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62578b-f6b6-4077-a314-f75ddb67dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtraction\n",
    "Variable.__sub__ = lambda self, other: Variable(self.f - other.f, self.d - other.d)\n",
    "\n",
    "# multiplication \n",
    "# notice that here the derivative already used both f and d\n",
    "Variable.__mul__ = lambda self, other: Variable(self.f * other.f, self.d * other.f + self.f * other.d)\n",
    "\n",
    "# division\n",
    "Variable.__truediv__ = lambda self, other: Variable(self.f / other.f, (self.d * other.f - self.f * other.d) / other.f ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91be932-b511-4de5-9aed-4119f0081066",
   "metadata": {},
   "source": [
    "Now let's test if our little implementation composes into a sensible derivative for a simple function:\n",
    "\n",
    "$$f(x, a, b) = \\frac{xa}{(x^2 + b)}$$\n",
    "\n",
    "Using the differentiation rules from above, we can (analitically) determine that the derivative of $f$ with respect to $x$ is:\n",
    "\n",
    "$$\\frac{\\partial f(x, a, b)}{\\partial x} = \\frac{a}{x^2 + b} - \\frac{2ax^2}{(x^2 + b)^2}$$\n",
    "\n",
    "If we set values for each of these variables, we can compute $f(x, a, b)$ and $\\displaystyle\\frac{\\partial f(x, a, b)}{\\partial x}$ numerically, to verify that the computation is correct. Let's set the values \n",
    "\n",
    "* $x = 24$\n",
    "* $a = 22$\n",
    "* $b = 2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f5d9b-0eb6-45da-a6c0-0be83430adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 24\n",
    "a = 22\n",
    "b = 2\n",
    "\n",
    "# f(x, a, b)\n",
    "f = x * a / (x * x + b)\n",
    "d_x = a/(x ** 2 + b) - (2 * a * x ** 2)/(x**2 + b)**2\n",
    "print(f\"f = {f}\")\n",
    "print(f\"d_x = {d_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321dbcea-3d3d-4ed6-81b9-9270bad24253",
   "metadata": {},
   "source": [
    "and now let's see if our differentiation engine computes these values correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f4467-29c9-48c6-bc78-70ec8f6b59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(24, 1)\n",
    "a = Variable(22, 0)\n",
    "b = Variable(2, 0)\n",
    "x * a / (x * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775885b-5e5d-45a9-afc6-c72149ac34ba",
   "metadata": {},
   "source": [
    "Now let's add more operators to our engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b5e22-276c-4089-8f35-30ae6d225bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable.sin = lambda self: Variable(np.sin(self.f), np.cos(self.f) + self.d)\n",
    "Variable.cos = lambda self: Variable(np.cos(self.f), -np.sin(self.f) + self.d)\n",
    "Variable.tan = lambda self: self.sin()/self.cos() # notice that by implementing this identity, we already get the derivative for free.\n",
    "Variable.log = lambda self: Variable(np.log(self.f), 1 / self.f * self.d)\n",
    "Variable.exp = lambda self: Variable(np.exp(self.f), np.exp(self.f) * self.d)\n",
    "\n",
    "def var_pow(self, number):\n",
    "    assert isinstance(number, (int, float))\n",
    "    return Variable(np.power(self.f, number), number * np.power(self.f, number -1) * self.d)\n",
    "\n",
    "Variable.__pow__ = var_pow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbcf45-9804-4aaa-a79a-c1c9c2b224b3",
   "metadata": {},
   "source": [
    "These are unary operators (they operate on the same variable). Notice how we always multiply by `self.d`. The reason for this is that we are representing each variable as a [dual number](https://en.wikipedia.org/wiki/Dual_number). We don't go very deep into that in this tutorial, but it suffices to know that they are similar to complex numbers, but they have the form $a + b\\varepsilon$, where $\\varepsilon$ is a symbol that satisfies $\\varepsilon^2 = 0$. We also need to know that extending the derivative to dual numbers, and we evaluate it at $x+1\\varepsilon$, we satisfy:\n",
    "\n",
    "$$f(x + 1\\varepsilon) = f(x) + f'(x)\\varepsilon$$\n",
    "\n",
    "This is the theoretical reason why forward-mode autodiff works. By computing a function and evaluating it on $x+1\\varepsilon$, we are automatically computing the derivative. \n",
    "\n",
    "Now, let's go back to the function we differentiated before:\n",
    "\n",
    "$$f(x, a, b) = \\frac{xa}{(x^2 + b)}$$\n",
    "\n",
    "If we want to determine the derivative of $f$ with respect to $a$ analitically, we get:\n",
    "\n",
    "$$\\frac{\\partial f(x, a, b)}{\\partial a} = \\frac{x}{x^2 + b}$$\n",
    "\n",
    "and with respect to $b$:\n",
    "\n",
    "$$\\frac{\\partial f(x, a, b)}{\\partial b} = \\frac{-xa}{(x^2 + b)^2}$$\n",
    "\n",
    "Numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b00bfa-3796-43e6-a7e4-56f0053dc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 24\n",
    "a = 22\n",
    "b = 2\n",
    "\n",
    "# f(x, a, b)\n",
    "f = x * a / (x * x + b)\n",
    "d_a = x/(x ** 2 + b)\n",
    "d_b = -x * a / (x ** 2 + b) ** 2\n",
    "\n",
    "print(f\"  f = {f}\")\n",
    "print(f\"d_x = {d_x}\") # we calculated this above\n",
    "print(f\"d_a = {d_a}\")\n",
    "print(f\"d_b = {d_b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db5a07-4493-4d06-b681-e9b7b45fd553",
   "metadata": {},
   "source": [
    "If we look again at how we defined the variables before, we see that in order to calculate the derivative with respect to a single variable, we need to set the gradient of each variable to 1 and the remaining ones to 0 each time, let's do that and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec5417-d0dc-4213-80b6-c55f9899922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return x * a / (x ** 2 + b) # because we implemented __pow__, we can now use **\n",
    "\n",
    "# derivative with respect to x\n",
    "x, a, b = Variable(24, 1), Variable(22, 0), Variable(2, 0)\n",
    "fx = f(x, a, b)\n",
    "\n",
    "# derivative with respect to a\n",
    "x, a, b = Variable(24, 0), Variable(22, 1), Variable(2, 0)\n",
    "fa = f(x, a, b)\n",
    "\n",
    "# derivative with respect to b\n",
    "x, a, b = Variable(24, 0), Variable(22, 0), Variable(2, 1)\n",
    "fb = f(x, a, b)\n",
    "\n",
    "print(f\"x: {fx}\")\n",
    "print(f\"a: {fa}\")\n",
    "print(f\"b: {fb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a015b-8924-49f0-b06f-47c76df64e12",
   "metadata": {},
   "source": [
    "The values of the partial derivatives match exactly! \n",
    "\n",
    "However, it becomes pretty evident that this is very inefficient if we want to calculate the derivative of a function with respect to multiple variables (which is exactly what we need for neural nets!). To overcome this limitation, we can implement reverse-mode autodiff, that requires two passes through the variables, but it's most efficient in the case of many inputs and few outputs (just what we need!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca775b59-a705-4d99-8e9b-b1ccc3bb4421",
   "metadata": {},
   "source": [
    "# Computational graph\n",
    "\n",
    "In our implementation of the forward-mode, we did not pay a lot of attention to the computational graph that is built to calculate the values. For reverse-mode, however, we need to pay close attention to it because if we want to traverse the graph in reverse order, we need to know which operator is computed when.\n",
    "\n",
    "Imagine we have 3 functions that depend on one another: $y = \\color{orange}{f(}\\color{green}{g(}\\color{red}{h(}\\color{blue}{x}\\color{red}{)}\\color{green}{)}\\color{orange}{)}$\n",
    "\n",
    "We can break this function down into its individual components:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_0 &= x \\\\\n",
    "w_1 &= \\color{red}{h(}w_0\\color{red}{)} \\\\\n",
    "w_2 &= \\color{green}{g(}w_1\\color{green}{)} \\\\\n",
    "w_3 &= \\color{orange}{f(}w_2\\color{orange}{)} = y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and differentiate using the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} &= \\frac{\\partial w_3}{\\partial w_0} \\\\\n",
    "&= \\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1}\\frac{\\partial w_1}{\\partial w_0} \\\\\n",
    "&= \\frac{\\partial \\color{orange}{f(}w_2\\color{orange}{)}}{\\partial w_2}\n",
    "   \\frac{\\partial \\color{green}{g(}w_1\\color{green}{)}}{\\partial w_1} \n",
    "   \\frac{\\partial \\color{red}{h(}w_0\\color{red}{)}}{\\partial w_1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can generalize this, and realize that we are computing the following recursive expression:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w_i}{\\partial w_0} = \\frac{\\partial w_i}{\\partial w_{i-1}}\\frac{\\partial w_{i-1}}{\\partial w_0}\n",
    "$$\n",
    "\n",
    "If we start at $i=3$, we end up with the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} &= \\left(\\left(\\frac{\\partial y}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\frac{\\partial y}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\left(\\frac{\\partial y}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\left(\\left(\\frac{\\partial y}{\\partial w_3}\\right)\\frac{\\partial w_3}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "## Gradient tape\n",
    "\n",
    "Breaking down a function into elementary operations is creating an _evaluation trace_ (also known as a \"tape\"). Let's look at the trace for the function\n",
    "\n",
    "$$y = f(x, a, b) = \\frac{xa}{(x^2 + b)}$$\n",
    "\n",
    "when we differentiate with respect to $x$ and evaluate at $(x, a, b) = (24, 22, 2)$.\n",
    "\n",
    "We first break down the function into elementary oprations, called _primals_, and simultaneously evaluate the derivative of the primals, called _tangents_. We'll associate these results with the intermediary variables $v_i$ and $\\dot{v_i}$, where\n",
    "\n",
    "$$\\dot{v_i} = \\frac{\\partial v_i}{\\partial x}$$\n",
    "\n",
    "Remember that in the forward-mode, we set the tangent of the target variable to 1 and the others to 0. These initial assigments are called the _seeds_. So for the derivative with respect to $x$, we set $\\dot{x} = 1$, $\\dot{a} = 0$, and $\\dot{b} = 0$.\n",
    "\n",
    "![](images/autodiff-tutorial/table.png)\n",
    "<!-- This renders horribly in jupyter, but here is the code to generate this table\n",
    "| Variable | Elementary operation | Primals         | Variable    | Elementary operation                           | Tangents |\n",
    "| -------- | -------------------- | -------         | --------    | --------------------                           | -------- |\n",
    "| $v_0$    | $=x$                 | $=24$           | $\\dot{v_0}$ | $=\\dot{x}$                                     | $=1$ |\n",
    "| $v_1$    | $=a$                 | $=22$           | $\\dot{v_1}$ | $=\\dot{a}$                                     | $=0$ |\n",
    "| $v_2$    | $=b$                 | $=2$            | $\\dot{v_2}$ | $=\\dot{b}$                                     | $=0$ |\n",
    "| $v_3$    | $=v_0 \\times v_1$    | $=24 \\times 22$ | $\\dot{v_3}$ | $=\\dot{v_0} \\times v_1 +  v_0 \\times \\dot{v_1}$ | $=1 \\times 22 + 24 \\times 0$ |\n",
    "| $v_4$    | $=v_0^2$             | $=24^2$         | $\\dot{v_4}$ | $=2 \\times v_0 \\times \\dot{v_0}$                | $=2 \\times 24 \\times 1$ |\n",
    "| $v_5$    | $=v_2 + v_4$         | $=2 + 576$      | $\\dot{v_5}$ | $=\\dot{v_2} + \\dot{v_4}$                        | $=0 + 48$ |\n",
    "| $v_6$    | $=v_3 / v_5$         | $=528 / 578$    | $\\dot{v_6}$ | $=(\\dot{v_3} \\times v_5 - v_3 \\times \\dot{v_5})/v_5^2$ | $=(22 \\times 578 - 528 \\times 48)/578^2$ |\n",
    "| $y$      | $=v_6$               | $=0.9134$       | $\\dot{y}$   | $=\\dot{v_6}$ | $=-0.037$ | \n",
    "-->\n",
    "\n",
    "We can visualize this evaluation trace by representing it as a computational graph (a directed acyclic graph):\n",
    "\n",
    "![](images/autodiff-tutorial/computation-graph.png)\n",
    "\n",
    "It should be pretty clear that constructing the graph explicitly is not required for forward-mode autodiff, but it is crucial for reverse-mode. For now, let's add some functionality to `Variable` so that we can trace the computation.\n",
    "\n",
    "First, we simply add a class member to store the `parents` of the variable in the graph, the `name` of the variable, and the operation `op` that we perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56a607-5148-416d-9319-31575dfa9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_id = 0 # global variable that we can use to name variables automatically\n",
    "\n",
    "class Variable():\n",
    "    def __init__(self, f, d = 0, children=(), op = ''):\n",
    "        global var_id\n",
    "        self.f = f # evaluation\n",
    "        self.d = d # tangent\n",
    "        self.op = op # operation\n",
    "        self.parents = set(children)\n",
    "        self.name = f\"v{var_id}\"\n",
    "        var_id += 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Var(f={self.f}, d={self.d}, name={self.name}, op={self.op}, n_parents={len(self.parents)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1ca8e-5442-42fa-aa92-3c810ca60693",
   "metadata": {},
   "source": [
    "And we can add all the operators we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab3766-36ff-4743-9b66-ea21f9cad349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary\n",
    "Variable.__add__ = lambda self, other: Variable(self.f + other.f, self.d + other.d, (self, other), \"+\")\n",
    "Variable.__sub__ = lambda self, other: Variable(self.f - other.f, self.d - other.d, (self, other), \"-\")\n",
    "Variable.__mul__ = lambda self, other: Variable(self.f * other.f, self.d * other.f + self.f * other.d, (self, other), \"*\")\n",
    "Variable.__truediv__ = lambda self, other: Variable(self.f / other.f, (self.d * other.f - self.f * other.d) / other.f ** 2, (self, other), \"/\")\n",
    "\n",
    "# unary\n",
    "Variable.sin = lambda self: Variable(np.sin(self.f), np.cos(self.f) + self.d, (self,), \"sin\")\n",
    "Variable.cos = lambda self: Variable(np.cos(self.f), -np.sin(self.f) + self.d, (self,), \"cos\")\n",
    "Variable.log = lambda self: Variable(np.log(self.f), 1 / self.f * self.d, (self,), \"log\")\n",
    "Variable.exp = lambda self: Variable(np.exp(self.f), np.exp(self.f) * self.d, (self,), \"exp\")\n",
    "\n",
    "def var_pow(self, number):\n",
    "    assert isinstance(number, (int, float))\n",
    "    return Variable(np.power(self.f, number), number * np.power(self.f, number -1) * self.d, (self,), f\"^{number}\")\n",
    "\n",
    "Variable.__pow__ = var_pow\n",
    "Variable.__neg__ = lambda self: self * -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e3a1a-7607-49d4-a7e0-567aea37a69a",
   "metadata": {},
   "source": [
    "We've added a bunch of stuff to the implementation, but so far it still implements the forward-mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b39e0-8ab0-4e0d-a04b-127e00764f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return x * a / (x ** 2 + b) # because we implemented __pow__, we can now use **\n",
    "\n",
    "# derivative with respect to x\n",
    "x, a, b = Variable(24, 1), Variable(22), Variable(2)\n",
    "fx = f(x, a, b)\n",
    "print(f\"fx : {fx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda88f3-4167-43d5-bd22-b7c7e99f0973",
   "metadata": {},
   "source": [
    "While this does offer a hint into the structure (we at least know that there are 2 parents for `fx`, we still don't see the full trace to know exactly all the steps that we took to make the calculation. For example, we can't compute $v_6$ before $v_3$ and $v_5$. $v_5$, in turn, cannot be computed before $v_4$. As we've seen in a previous lab, this can be done using the topological sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba3ff5-3b51-48df-a60b-c72e4af91e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(self):\n",
    "    topo_sorted = []\n",
    "    visited = set()\n",
    "    \n",
    "    def topo_sort(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for parent in v.parents:\n",
    "                topo_sort(parent)\n",
    "            topo_sorted.append(v)\n",
    "    \n",
    "    topo_sort(self)\n",
    "    return topo_sorted\n",
    "Variable.graph = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705dc30-2967-47ba-90ff-afabf21cf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e429d-6ef5-476e-afa6-6b954507ec55",
   "metadata": {},
   "source": [
    "Although it should be pretty clear that the graph is stored in memor using the links we built using the variables, we are missing a way to visualize the actual graph. We will use `graphviz` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb375e-5d68-44e0-986f-0addfe73b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05456740-423e-4c48-aed8-0aa6af973941",
   "metadata": {},
   "source": [
    "To visualize the graph, we need the nodes as well as the edges. We can collect these with a slightly modified version of our `graph` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5556fb5-bae7-4ac0-8ef4-1c9791be5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(self):\n",
    "    nodes, edges = set(), set()\n",
    "    def traverse(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for parent in v.parents:\n",
    "                edges.add((parent, v))\n",
    "                traverse(parent)\n",
    "    traverse(self)\n",
    "    return nodes, edges\n",
    "\n",
    "Variable.trace = trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac61c04-27fb-4b9d-973a-97dace2293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(root, orientation=\"LR\"):\n",
    "    assert orientation in [\"LR\", \"TB\"] # left to right or top to bottom\n",
    "    nodes, edges = root.trace()\n",
    "    \n",
    "    dot = Digraph(graph_attr={\"rankdir\":orientation})\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = f\"{n.name}| f {n.f:.3f}| d {n.d:.3f}\", shape=\"record\")\n",
    "        if n.op:\n",
    "            dot.node(name=str(id(n)) + n.op, label=n.op)\n",
    "            dot.edge(str(id(n)) + n.op, str(id(n)))\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c084a2-767d-44b3-8998-62a02c7ed4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d2ffe-2bd8-4b2f-b73e-d1b5df4116a3",
   "metadata": {},
   "source": [
    "It looks remarkably like the graph we saw earlier. But we are not calculating the gradients in reverse-mode yet. Let's do that now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fdb0f-005c-4882-9464-e1cfe22255af",
   "metadata": {},
   "source": [
    "# Reverse-mode\n",
    "\n",
    "So far, we've implemented the forward-mode and a way to trace the computational graph that computes all the primals and tangents.\n",
    "Now, we simply need to compute the gradients by starting from the output of the evaluation trace, and traverse towards the inputs.\n",
    "Luckily, adding this to our implementation is not difficult, as we need to:\n",
    "1. Compute the function evaluation to start computing the gradient in the reverse direction\n",
    "2. The primal trace graph to traverse backwards\n",
    "3. Implement the operation we have to perform to compute the gradient.\n",
    "\n",
    "1 and 2 are implemented in the forward-mode. And with minimal modifications. First, we no longer need the forward accumulation of the gradients, and therefore we don't need to compute this. Then, we need to remember which operations we need to perform in the reverse direction. We simply store the function as a member of the node to run when we do the backward pass. And that's it!\n",
    "\n",
    "First, let's look at the differences between forward-mode and reverse-mode. In reverse-mode, each intermediate variable $v_i$ is associated with an _adjoint_ $\\bar{v}_i$ (note that this is different from the _tangent_ in forward mode:\n",
    "\n",
    "$$\\bar{v}_i = \\frac{\\partial y_j}{\\partial v_i}$$\n",
    "\n",
    "The adjoint represents the sensitivity of an output $y_j$ with respect to changes in $v_i$. Again, imagine we have 3 functions that depend on one another: $y = \\color{orange}{f(}\\color{green}{g(}\\color{red}{h(}\\color{blue}{x}\\color{red}{)}\\color{green}{)}\\color{orange}{)}$\n",
    "\n",
    "We can break this function down into its individual components:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_0 &= x \\\\\n",
    "w_1 &= \\color{red}{h(}w_0\\color{red}{)} \\\\\n",
    "w_2 &= \\color{green}{g(}w_1\\color{green}{)} \\\\\n",
    "w_3 &= \\color{orange}{f(}w_2\\color{orange}{)} = y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and differentiate using the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} &= \\frac{\\partial w_3}{\\partial w_0} \\\\\n",
    "&= \\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1}\\frac{\\partial w_1}{\\partial w_0} \\\\\n",
    "&= \\frac{\\partial \\color{orange}{f(}w_2\\color{orange}{)}}{\\partial w_2}\n",
    "   \\frac{\\partial \\color{green}{g(}w_1\\color{green}{)}}{\\partial w_1} \n",
    "   \\frac{\\partial \\color{red}{h(}w_0\\color{red}{)}}{\\partial w_1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can generalize this, and realize that we are computing the following recursive expression:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w_3}{\\partial w_i} = \\frac{\\partial w_3}{\\partial w_{i+1}}\\frac{\\partial w_{i+1}}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "Again, starting from $i=0$, we achieve the nasty expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} &= \\left(\\left(\\frac{\\partial y}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\frac{\\partial y}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\left(\\frac{\\partial y}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "&= \\left(\\left(\\left(\\left(\\left(\\frac{\\partial y}{\\partial w_3}\\right)\\frac{\\partial w_3}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial w_0}\\right)\\frac{\\partial w_0}{\\partial x}\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's compute the derivatives in reverse-mode for our function:\n",
    "\n",
    "$$y = f(x, a, b) = \\frac{xa}{(x^2 + b)}$$\n",
    "\n",
    "![](images/autodiff-tutorial/forward.png)\n",
    "\n",
    "![](images/autodiff-tutorial/backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c54765-bc75-41cc-bf42-a641bae03517",
   "metadata": {},
   "source": [
    "Note that when we ran the computations in reverse-mode, we ended up with the partial derivatives for all inputs! \n",
    "\n",
    "A case such as this function, where the output is a scalar is a special case of automatic differentiation that we know as backpropagation!\n",
    "\n",
    "Now, let's code this into our differentiation engine. For each node (operation), we will define a `backward` opration using the current accumulated gradient `d` of `self` and `other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc5fa3-ce61-4edb-b60f-0878e262dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_id = 0\n",
    "\n",
    "class Variable():\n",
    "    def __init__(self, f, children = (), op = ''):\n",
    "        global var_id\n",
    "        self.f = f # evaluation\n",
    "        self.d = 0 # adjoint\n",
    "        self.op = op \n",
    "        self.backward_fn = lambda: None # backwards function\n",
    "        self.parents = set(children) \n",
    "        self.parents = set(children)\n",
    "        self.name = f\"v{var_id}\"\n",
    "        var_id += 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Var(f={self.f}, d={self.d}, name={self.name}, op={self.op}, n_parents={len(self.parents)}), is_leaf={'False' if len(self.parents) else 'True'}\"\n",
    "        \n",
    "    \n",
    "    # I will add only the operators we need for the function we're studying, but feel free to add more operators (reuse the ones implemented in the forward-mode)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.f + other.f, (self, other), '+')\n",
    "\n",
    "        def _backward_fn():\n",
    "            self.d  += out.d\n",
    "            other.d += out.d\n",
    "        out.backward_fn = _backward_fn\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.f - other.f, (self, other), '-')\n",
    "\n",
    "        def _backward_fn():\n",
    "            self.d  += out.d\n",
    "            other.d -= out.d\n",
    "        out.backward_fn = _backward_fn\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.f * other.f, (self, other), '*')\n",
    "\n",
    "        def _backward_fn():\n",
    "            self.d  += other.f * out.d\n",
    "            other.d += self.f * out.d\n",
    "        out.backward_fn = _backward_fn\n",
    "\n",
    "        return out\n",
    "    def __truediv__(self, other):\n",
    "        out = Variable(self.f / other.f, (self, other), \"/\")\n",
    "        \n",
    "        def _backward_fn():\n",
    "            self.d  += 1 / other.f * out.d\n",
    "            other.d += -self.f / other.f ** 2 * out.d\n",
    "\n",
    "        out.backward_fn = _backward_fn\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Variable(self.f**other, (self,), f'^{other}')\n",
    "\n",
    "        def _backward_fn():\n",
    "            self.d += (other * self.f**(other-1)) * out.d\n",
    "        out.backward_fn = _backward_fn\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        tape = self.graph()\n",
    "        \n",
    "        self.d = 1\n",
    "        \n",
    "        for v in reversed(tape):\n",
    "            print(v)\n",
    "            v.backward_fn()\n",
    "\n",
    "# We redefined this class, let's reuse the implementation from before            \n",
    "Variable.graph = graph\n",
    "Variable.trace = trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13eb56-d221-4535-b261-7f736d579277",
   "metadata": {},
   "source": [
    "Now, let's compute the forward pass as usual (the gradients are not populated yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf3c97-4925-4396-b50c-17d63bda2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return x * a / (x ** 2 + b) # because we implemented __pow__, we can now use **\n",
    "\n",
    "x, a, b = Variable(24), Variable(22), Variable(2)\n",
    "y = f(x, a, b)\n",
    "print(f\"y : {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28793cb-0685-4706-b2c7-a704d93f27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9539565-b805-4545-9b05-0e1fc4446f59",
   "metadata": {},
   "source": [
    "To finish, let's draw the graph in reverse mode, so we can observe what's going on without having to read the list in a non-intuitive order, this function is the same, only we reverse the direction of the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b2b5a-e298-4a84-8adc-5cba750e3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(root, orientation=\"RL\"):\n",
    "    assert orientation in [\"LR\", \"TB\", \"RL\"] # left to right, top to bottom, or right to left\n",
    "    nodes, edges = root.trace()\n",
    "    \n",
    "    dot = Digraph(graph_attr={\"rankdir\":orientation})\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = f\"{n.name}| f {n.f:.3f}| d {n.d:.3f}\", shape=\"record\")\n",
    "        if n.op:\n",
    "            dot.node(name=str(id(n)) + n.op, label=n.op)\n",
    "            dot.edge(str(id(n)), str(id(n)) + n.op)\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n2)) + n2.op, str(id(n1)))\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2854cfa-3ff8-4680-bfbd-1b4633addb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913f586-5d98-43f0-a2e1-2e26a8281d70",
   "metadata": {},
   "source": [
    "# Next steps (optional exercises for home, highly recommended)\n",
    "\n",
    "You now have a differentiation engine to play around with. \n",
    "\n",
    "Try to implement\n",
    "\n",
    "* a Linear Regression\n",
    "* a Single Perceptron\n",
    "* a MultiLayer Perceptron\n",
    "\n",
    "By composing these functions, you should be able to do this. I highly encourage you to try and think about it carefully before looking for references, it's a great exercise!\n",
    "\n",
    "If you're totally lost, have a read at the [micrograd demo](https://github.com/karpathy/micrograd/blob/master/demo.ipynb). That one uses a smaller engine, but with a very similar API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
