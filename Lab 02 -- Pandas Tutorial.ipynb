{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quiet-boards",
   "metadata": {},
   "source": [
    "# Pandas Tutorial\n",
    "\n",
    "In this lab session, we'll go through some useful ways to use [Pandas](https://pandas.pydata.org/) for data science. As is the case with the previous tutorial, this is NOT a comprehensive Pandas tutorial, but rather a collection of tips and useful fucntions that I expect you to use for the labs, exams, and courseworks of the course.\n",
    "\n",
    "* This is a self-paced tutorial, but make sure to take some time on this week to get familiar with the concepts.\n",
    "* If you find any part of this tutorial complicated, please prepare a list of questions for the beginning of the next lab.\n",
    "* The usual: the canonical answer to any of you questions is probably contained in the [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) and [Documentation](https://pandas.pydata.org/docs/)\n",
    "* Similar to NumPy, Pandas is one of the essential tools for data science with Python, and if you're serious about data science as a career, you should take as long as you need to become proficient with the tool. I believe the best place to start is the [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html) tutorial.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Series](#Series)\n",
    "2. [DataFrames](#DataFrames)\n",
    "3. [Indexing](#Indexing)\n",
    "4. [Long vs narrow data](#Long-vs-narrow-data)\n",
    "5. [Useful patterns](#Useful-patterns)\n",
    "6. [Copies and Views](#Copies-and-Views)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you're using Google Colab\n",
    "\n",
    "!git clone https://github.com/torresmateo/fgv-class-2022.git\n",
    "!cp -r fgv-class-2022/images .\n",
    "!cp -r fgv-class-2022/tutorials .\n",
    "!cp -r fgv-class-2022/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tutorials.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e199cac-5490-4965-93cb-7e118d80ac1b",
   "metadata": {},
   "source": [
    "## Series\n",
    "\n",
    "Pandas, like NumPy, provides most of its functionality by providing data structures and functions that operate on such structures. The first data structure is the [`Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html). \n",
    "\n",
    "At the very core, a Series is a one-dimensional NumPy ndarray that supports axis labels. This immediately offers some advantages when compared to a ndarray. To see some of these advantages, let's create some Series below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fb3b7-8573-40fc-b33e-e72e175ba4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by creating an alphabetic index for our ndarray\n",
    "index = list(alpha_range(5))\n",
    "\n",
    "# then, let's create a one-dimensional ndarray that will hold the data for our Series\n",
    "data = np.arange(5)\n",
    "\n",
    "print(f\"index: {index}\")\n",
    "print(f\"data: {data}\")\n",
    "\n",
    "# Now we can create a that uses the index to name each of the componens of our one-dimensional ndarray\n",
    "s1 = pd.Series(data, index=index)\n",
    "\n",
    "print(f\"Series:\\n{s1}\")\n",
    "\n",
    "# By default the index is simply a range that enumerates each component\n",
    "rng = np.random.default_rng(0)\n",
    "s2 = pd.Series(rng.random(5))\n",
    "\n",
    "print(f\"Series with default index:\\n{s2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204fe15-f943-4b81-8202-c65cb6c7557c",
   "metadata": {},
   "source": [
    "notice how the `Series` object not only holds the data and the index, but also some metadata such as the type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8d424-4e9d-49fc-a309-d072f4a14163",
   "metadata": {},
   "source": [
    "### `Series` as a `ndarray`\n",
    "\n",
    "`Series` objects are ndarray-like. This means that most NumPy operators will operate on a `Series`, including the slicing operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae01929-17a3-4910-8125-288c0851d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s1:\\n{s1}\")\n",
    "\n",
    "# Notice how slicing this series to reverse it works exactly like it does for a ndarray,\n",
    "# but it reverses also the index. This is extremely useful for keeping sorted indices consistent\n",
    "# with the data. \n",
    "print(f\"\\ns1[-1::-1]:\\n{s1[-1::-1]}\")\n",
    "\n",
    "# It also support the NumPy advanced slicing\n",
    "# In this example, you can see also that the index is not necessarily unique.\n",
    "\n",
    "print(f\"\\ns1[[0,1,0,3]]:\\n{s1[[0,1,0,3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dfe336-6ad8-4b39-a8e0-be2682956c80",
   "metadata": {},
   "source": [
    "### `Series` as a Python `dict`\n",
    "\n",
    "A convenient way to access and modify values on a `Series` is using the Python `dict` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4379ca4-231c-4f94-8b6c-84ac1d4735c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s1[\\\"a\\\"]: {s1['a']}\")\n",
    "\n",
    "s1[\"a\"] = 20\n",
    "\n",
    "print(f's1[\"a\"]: {s1[\"a\"]}')\n",
    "\n",
    "print(\"\\nCase with non-unique indices\\n\")\n",
    "\n",
    "\n",
    "# We create now a series with distinct values, but repeated indices\n",
    "s3 = pd.Series(np.arange(5), index=[\"a\", \"b\", \"a\", \"b\", \"a\"])\n",
    "\n",
    "print(f\"Original Series s3:\\n{s3}\")\n",
    "\n",
    "s3[\"a\"] = 0\n",
    "\n",
    "print(f'\\nAfter modifying the \"a\" key s3:\\n{s3}')\n",
    "\n",
    "# Notice also that using repeated indices will return all values that match the selected index\n",
    "print(f'\\ns3[\"b\"]:\\n{s3[\"b\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76470379-b1e6-44dc-8c7a-efe5f5eaf978",
   "metadata": {},
   "source": [
    "### `Series` operations\n",
    "\n",
    "Like `ndarrays`, `Series` offers a large collection of functions and operators for a single or multiple series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58526974-e63c-4a68-b61f-0bbcfa20e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s1:\\n{s1}\\n\")\n",
    "print(f\"s2:\\n{s2}\\n\")\n",
    "print(f\"s3:\\n{s3}\\n\")\n",
    "\n",
    "print(f\"s1.mean(): {s1.mean()}\\n\")\n",
    "print(f\"s2.median(): {s2.median()}\\n\")\n",
    "print(f\"pd.concat([s1, s2]):\\n{pd.concat([s1, s2])}\\n\")\n",
    "\n",
    "# Notices this special case, where s1 and s3 share only some of the indices. \n",
    "# The rules that establish how the operators will be mapped is known as \"alignment\", \n",
    "# and we'll see it later in the tutorial\n",
    "print(f\"s1 + s3:\\n{s1 + s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699c8e3-5fbe-45b9-9e50-885ba1517d0d",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "While `Series` is the basic data structure of Pandas, by far the most used data structure is the Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "The easiest (although incorrect) way to think about a `DataFrame` is a 2-dimensional `Series` object. This is mostly because a `DataFrame` can be heterogeneous on the types of each of the series that compose it.\n",
    "\n",
    "For those used to dealing with spreadsheets and SQL tables, a `DataFrame` will feel quite familiar. However, much like a `ndarray` is not a mathematical vector, a `DataFrame` is NOT a 2-dimensional ndarray, nor a SQL table, nor an Excel spreadsheet, even though many operations that apply to those objects apply to the `DataFrame` in very similar ways. A `DataFrame` is a unique data structure that inherits many properties of `Series` and `ndarray`.\n",
    "\n",
    "A special characteristic of a `DataFrame` is that its two dimensions are _labeled_, meaning you have a `Series`-like index on the rows and the columns of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654571d4-2f68-4d5f-af41-bb2e8733f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our first DataFrame as a dictionary of Series\n",
    "s1 = pd.Series(np.arange(5), index=alpha_range(5))\n",
    "s2 = pd.Series(np.arange(3), index=alpha_range(3))\n",
    "s3 = pd.Series(np.arange(4), index=alpha_range(4))\n",
    "\n",
    "print(f\"s1:\\n{s1}\\n\")\n",
    "print(f\"s2:\\n{s2}\\n\")\n",
    "print(f\"s3:\\n{s3}\\n\")\n",
    "\n",
    "data = {\n",
    "    'one': s1, \n",
    "    'two': s2, \n",
    "    'three': s3\n",
    "}\n",
    "# we simply pass this dictionary to the constructor, and we get our first DataFrame\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# if we print it, we get a text table\n",
    "print(df)\n",
    "\n",
    "# Because we're in a notebook, we can inspect the variable \"as is\" and get a web-friendly table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d91bc0-95f7-4ea9-a04b-c4f44144f5b0",
   "metadata": {},
   "source": [
    "### Data Alignment in Pandas\n",
    "\n",
    "The `DataFrame` we created above is convenient to explain the concept of data alignment in Pandas.\n",
    "\n",
    "Notice how `s1` is the only `Series` with 5 indices (`a` to `e`). `s2` and `s3` share 3, and 4 of those letter respectively.\n",
    "\n",
    "Because the underlying representation a `DataFrame` must have _something_ on the coordinates where data was not provided, it will fill this empty spaces with the default NumPy representation for missing data: `np.nan` (printed as `NaN`)\n",
    "\n",
    "Notice that the `int64` data type was changed to something else for columns `two` and `three`, this is because `np.nan` is defined as a `float` value in NumPy, and therefore all the Series was cast into `float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f9437-744f-4721-a807-31a7b57406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6309c39-2897-4ff6-b56c-fe5e588a1ef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The `Series` in a `DataFrame`\n",
    "\n",
    "let's dissect the `DataFrame` s bit more, and see if we can reveal some more of the underlying data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd816c-6eb6-4efb-b1b4-763172743749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df[\"one\"]:\\n{df[\"one\"]}\\n')\n",
    "print(type(df[\"one\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec9bb9-ffdb-4de7-b438-4fecef6acf8d",
   "metadata": {},
   "source": [
    "We can see that a column is actually a `Series`, let's see one of the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c1bdf-d44a-4efb-bbef-8cce68e99878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df.loc[\"d\"]:\\n{df.loc[\"d\"]}\\n')\n",
    "print(type(df.loc[\"d\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01aedc-c71c-4f93-a411-61b7ec45be61",
   "metadata": {},
   "source": [
    "Again, when we extract a row, we get a `Series`. Notice, however that the `3` on the `one` index of this `Series` was cast into `3.0` (`int64` -> `float64`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf85df-7468-48f2-a1fb-17ec4f572240",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "We've seen some Numpy-like indexing above, which are the most intuitive and straightforward ways to select subsets of the data in Pandas. \n",
    "\n",
    "During the course, we won't need much advanced indexing, but it's very important that you read the [User Guide](https://pandas.pydata.org/docs/user_guide/indexing.html), as the flexibility of data selection in Pandas can become very complex an confusing. Here, I simply provide some useful examples that should be useful for the course.\n",
    "\n",
    "To start, let's load a dataset of Clinical Trials downloaded in 2020 from [ClinicalTrials.gov](https://clinicaltrials.gov/) and that were related to COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8226dfc-c569-4c88-b6f8-df2af38f9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_trials = pd.read_table('data/clinical_trials-raw.tsv')\n",
    "clinical_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74dea0-953a-4ab2-8dec-cfc0c2643f48",
   "metadata": {},
   "source": [
    "So, at a glance, we know we have 4645 Clinical Trials, and we can see some of the columns on the table representation. but this does not tell us very useful things so far. Let's inspect the `DataFrame` further. Let's get an idea of all the columns and see if we can make some useful selections to answer some questions about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c02753-a89c-4392-b703-027b448af254",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_trials.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f131e2d-e443-40d7-8d7b-3840492ded15",
   "metadata": {},
   "source": [
    "We observe that most of the columns are strings (which are automatically cast to `object`. Some of these columns seem to have been cast erroneusly thoug, such as `Age`, which could be an integer. Let's inspect that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf8c5d-e610-4dc4-a359-1c9ba8203cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_trials[\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b61c4-6254-4355-b3f6-ede0aa468010",
   "metadata": {},
   "source": [
    "This explains it, it's not a single number, but a description of a _range_ of ages of the patients involved. We also see that there is a categorical value between parentheses in the field. If we can extract this into a new Series, we could visualize the Age distribution for COVID-related clinical trials. Let's extract these values, and count the number of occurences of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3dbff-f80d-4872-8d5f-b17997d915d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinical_trials[\"Age category\"] = clinical_trials[\"Age\"].str.split('(').str[1].str.split(')').str[0]\n",
    "\n",
    "# we can get the values to the console\n",
    "print(clinical_trials[\"Age category\"].value_counts())\n",
    "\n",
    "# or use a librart to make a chart with the data, which could be more intuitive for visual comparison\n",
    "sns.countplot(y=clinical_trials[\"Age category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3533d30-5fc7-4136-bc5c-f3ba8557f690",
   "metadata": {},
   "source": [
    "This is interesting, simply by exploring the data, we can already conclude that around April 2020, most clinical trials were targetting Adults and Older Adults rather than children. The analysis is not as clean as it could be, because some of the categories are still mixed together. I leave the task of doing further cleaning of this field to you as an exercise (such cleaning was part of the Python Assessment at the beginning of the course)\n",
    "\n",
    "Let's pick another column and do a similar analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55879b-bbf9-447d-a6e9-c3c139066065",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=clinical_trials[\"Phases\"],\n",
    "              order=clinical_trials[\"Phases\"].value_counts().index\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b49db4-55ca-4779-ae03-a9a741db4d64",
   "metadata": {},
   "source": [
    "## Long vs narrow data\n",
    "\n",
    "The previous dataset was in record, or \"wide\" format. That format is intuitive because we can mentally map a row to a \"record\" with properties.\n",
    "This can get complicated however, if a particular field should contain a list of values instead of a single value. The previous dataset decided to \n",
    "use a secondary (and I've tertiary, and even quaternary) level of separation, with another separation character.\n",
    "\n",
    "A popular alternative, due to its simplicity, is the Long format, which spreads a single record across multiple rows, and this allows us to avoid dealing with so many separators... let's look at one example. I'm going to load a dataset in long format, and I will create a wide datastet by [pivoting](https://pandas.pydata.org/docs/user_guide/reshaping.html) it (although in this case, with some data loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5e406-3cb9-4292-a5a7-242cf8486239",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank = pd.read_pickle('data/drugbank.pkl')\n",
    "# Let's do a simple (but lossy) pivot of the data\n",
    "db_long = drugbank.pivot_table(index='DrugBank ID', columns='variable', values='value', aggfunc=min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166483f-421b-4bdc-86b3-3d93e4f06fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9e15b-ffa5-4b3f-8a08-c0619ed94c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6b5f4-8f03-4be5-a458-c13c48c45a50",
   "metadata": {},
   "source": [
    "We can see that `drugbank` has only 3 columns, and that the values of the `DrugBank ID` and `variable` columns are repeated. This, however, is very useful to quickly aggregate statistics in Pandas, without going through the cleaning process of separating fields with multiple values. For instance, let's make a count plot of the groups of the drug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4dd69-a427-43b1-b509-ce49e0e0f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by making a boolean mask to select only the rows with the property we want\n",
    "condition = drugbank['variable'] == 'Group'\n",
    "sns.countplot(y=drugbank.loc[condition, 'value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d07c8-6b71-4108-9c72-39d5ca390120",
   "metadata": {},
   "source": [
    "Because a drug can be in multiple groups simultaneously, getting the same result from the wide format is way more difficult (actually impossible if we use the pivot I did above, because we kept only one of the groups for each drug). \n",
    "\n",
    "Now, let's do a more complex calculation. Let's extract the distribution of drug targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21da4b9-fc0f-481d-aec0-27f658bf6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = drugbank['variable'] == 'Target'\n",
    "\n",
    "# let's make a wider figure, so it's readable\n",
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "# we make the plot (notice the call to value_counts\n",
    "sns.countplot(x=drugbank.loc[condition, 'DrugBank ID'].value_counts(), ax=ax)\n",
    "\n",
    "# set a logarithmic scale for the plot\n",
    "ax.set_yscale('log') \n",
    "\n",
    "# set sensible labels for the axis\n",
    "ax.set_ylabel('count (log)') \n",
    "ax.set_xlabel('Target counts') \n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400beb1-42c2-4e29-84df-28663d37b0b7",
   "metadata": {},
   "source": [
    "## Useful patterns\n",
    "\n",
    "The usage of Pandas will be unique for every project and particular characteristics of the dataset you're dealing with on that context.\n",
    "\n",
    "However, there are some patterns that will be useful in most cases. This is a very non-comprehensive set of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b746f5-7666-4e0e-acf9-b0906184277d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading from non-standard formats\n",
    "\n",
    "Often, it is impossible to get a \"clean\" dataset where every field is separated perfectly, and very often you will need to write a little parser for the data.\n",
    "\n",
    "Let's have a look at the file contained in `data/example_data.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/example_data.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7b98d-4ab8-4a19-a734-7328fa3855ae",
   "metadata": {},
   "source": [
    "The header describes the format of the file, and it's pretty simple to understand. \n",
    "\n",
    "However, we'll have some trouble attempting to read this file using the built-in functions in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_table('data/example_data.txt', comment='!', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78519ab8-2bca-45dd-abf4-5ae15dffd0f9",
   "metadata": {},
   "source": [
    "A file with this format is famously (or infamously) the output of HMMER, a tool for aligning protein sequences based on Hidden Markov Models. The solution in this case, is to parse the lines ourselves, and build a dictionary or list of dataclasses to populate out `DataFrame`. For simplicity, I will write a simple parser that does not require a dataclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5bccd-71a9-4ac5-ad5d-f63ee6b5357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our simple parser splits the line up to 4 times (creating 5 pieces)\n",
    "def parse_line(l):\n",
    "    fields = l.strip().split(' ', 4)\n",
    "    for i in range(4):\n",
    "        fields[i] = float(fields[i])\n",
    "    return fields\n",
    "\n",
    "columns = ['field1', 'field2', 'field3', 'field4', 'field5']\n",
    "data = {c:[] for c in columns}\n",
    "\n",
    "skip_header = True\n",
    "with open('data/example_data.txt') as f:\n",
    "    for line in f:\n",
    "        if line[0] == '!':\n",
    "            continue\n",
    "        if skip_header:\n",
    "            skip_header = False\n",
    "            continue\n",
    "        fields = parse_line(line)\n",
    "        for c, f in zip(columns, fields):\n",
    "            data[c].append(f)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f0257-f993-4f9c-a6da-c81b6ffe3cf7",
   "metadata": {},
   "source": [
    "## Copies and Views\n",
    "\n",
    "Like NumPy, the underlying assumption for Pandas data structures is that you get a view of the object when you slice it, rather than a copy. In Pandas, however, we often use very complex selection patterns, that force Pandas to return a copy instead. \n",
    "\n",
    "A very common mistake is to \"update\" some values on a copy instead of a view, and assume that the original dataset is being modified. Dealing with this can be quite tricky, but fortunately, we won't need to do such kind of data cleaning during the course.\n",
    "\n",
    "Nevertheless, I think it's quite important that you are aware of this, and to point you to the [relevant documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#returning-a-view-versus-a-copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
