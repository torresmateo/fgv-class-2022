{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be049f25",
   "metadata": {},
   "source": [
    "# Random Forest for for Regression\n",
    "\n",
    "In this lab, we will be using the solution from the last lab, and we will extend it to implement a random forest for regression. To simplify the process, we will be using the same dataset, and therefore code to load it and splitting it into training and testing is provided below.\n",
    "\n",
    "As a reminder we are making a dataset where the features are\n",
    "\n",
    "* `variance`\n",
    "* `skewness`\n",
    "* `curtosis`\n",
    "\n",
    "and the target variable is\n",
    "\n",
    "* `entropy`\n",
    "\n",
    "For this lab, we simply ignore the `class` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt', names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])\n",
    "\n",
    "X = data.loc[:, [\"variance\", \"skewness\", \"curtosis\"]].values\n",
    "y = data[\"entropy\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f0846-bb3b-4e5b-bde2-4db0646df7a5",
   "metadata": {},
   "source": [
    "## DecisionTree Regressor \n",
    "\n",
    "In the following cell, I simply copy the solution from the previous lab, and I turn it into a scikit-learn regressor for convenience. I also copy the function to print the tree, as part of the regressor.\n",
    "\n",
    "> This code SEEMS extremely long, but you can delete the documentation lines and see that it's actually not too bad <span style=\"font-size:20pt;\">ðŸ˜ƒ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462ba4ed-f427-4d91-9dca-8522f70e788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class DTRegressor(RegressorMixin):\n",
    "\n",
    "    \"\"\"\n",
    "    CART Decision Tree Regressor\n",
    "    \n",
    "    This is not an efficient implementation, but it should serve to learn how to put together \n",
    "    a simple decision tree for regression.\n",
    "    \n",
    "    All features are assumed to be numeric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_samples : int\n",
    "        parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "    max_depth : int\n",
    "        parameter for stopping criterion if a node belongs to this depth\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 min_samples,\n",
    "                 max_depth):\n",
    "        self.min_samples_ = min_samples\n",
    "        self.max_depth_ = max_depth\n",
    "        self.root_ = None # placeholder to hold the root node for predictions\n",
    "    \n",
    "    def print_tree(self, depth):\n",
    "        if 'value' in node.keys():\n",
    "            print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "        else:\n",
    "            print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "            print_tree(node['left'], depth+1)\n",
    "            print_tree(node['right'], depth+1)\n",
    "\n",
    "    def recursive_growth(self, node, current_depth, X, y):\n",
    "        \"\"\"\n",
    "        Recursively grows a decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dictionary\n",
    "            If the node is terminal, it contains only the \"value\" key, which determines the value to be used as a prediction.\n",
    "            If the node is not terminal, the dictionary has the structure defined by `get_split`\n",
    "        depth : int\n",
    "            current distance from the root\n",
    "        X : array (n_samples, n_features)\n",
    "            features (full dataset)\n",
    "        y : array (n_samples, )\n",
    "            labels (full dataset)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "        is the mean of the target variable\n",
    "\n",
    "        'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "        from higher levels of the tree:\n",
    "        'left' corresponds to the 'low_region' key, and 'right' to the 'high_region' key\n",
    "        \"\"\"\n",
    "        if 'low_region' in node.keys(): # not a terminal node\n",
    "            lo = node['low_region']\n",
    "            hi = node['high_region']\n",
    "            # process left\n",
    "            if len(lo) <= self.min_samples_ or current_depth == self.max_depth_:\n",
    "                node['left'] = {'value':y[lo].mean()}\n",
    "            else:\n",
    "                node['left'] = self.get_split(X[lo], y[lo])\n",
    "                self.recursive_growth(node['left'], current_depth + 1, X, y)\n",
    "\n",
    "            # process right\n",
    "            if len(hi) <= self.min_samples_ or current_depth == self.max_depth_:\n",
    "                node['right'] = {'value':y[lo].mean()}\n",
    "            else:\n",
    "                node['right'] = self.get_split(X[hi], y[hi])\n",
    "                self.recursive_growth(node['right'], current_depth + 1, X, y)\n",
    "\n",
    "    def get_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array (n_samples, n_features)\n",
    "            features \n",
    "        y : array (n_samples, )\n",
    "            labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decision : dictionary\n",
    "            keys are:\n",
    "            * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "            * 'tau' -> the threshold used to make the split\n",
    "            * 'low_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "            * 'high_region' -> indices not in `low_region`\n",
    "        \"\"\"\n",
    "        best_criterion = float(\"inf\") # unreasonably high Gini Index\n",
    "        best_feature_index = None\n",
    "        best_tau = None\n",
    "        best_lo = None\n",
    "        best_hi = None\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            for tau in X[:, feature_index]:\n",
    "                lo, hi = self.split_region(X, feature_index, tau)\n",
    "                criterion = (self.regression_criterion(y[lo]) + \n",
    "                             self.regression_criterion(y[hi]))\n",
    "                if criterion < best_criterion:\n",
    "                    best_criterion = criterion\n",
    "                    best_feature_index = feature_index\n",
    "                    best_tau = tau\n",
    "                    best_lo = lo\n",
    "                    best_hi = hi\n",
    "        return {\n",
    "            'feature_index': best_feature_index,\n",
    "            'tau': best_tau,\n",
    "            'low_region' : best_lo,\n",
    "            'high_region' : best_hi,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def split_region(region, feature_index, tau):\n",
    "        \"\"\"\n",
    "        Given a region, splits it based on the feature indicated by\n",
    "        `feature_index`, the region will be split in two, where\n",
    "        one side will contain all points with the feature with values \n",
    "        lower than `tau`, and the other split will contain the \n",
    "        remaining datapoints.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region : array of size (n_samples, n_features)\n",
    "            a partition of the dataset (or the full dataset) to be split\n",
    "        feature_index : int\n",
    "            the index of the feature (column of the region array) used to make this partition\n",
    "        tau : float\n",
    "            The threshold used to make this partition\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        low_partition : array\n",
    "            indices of the datapoints in `region` where feature < `tau`\n",
    "        high_partition : array\n",
    "            indices of the datapoints in `region` where feature >= `tau` \n",
    "        \"\"\"\n",
    "        return np.where(region[:, feature_index] < tau)[0], np.where(region[:, feature_index] >= tau)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def regression_criterion(region):\n",
    "        \"\"\"\n",
    "        Implements the sum of squared error criterion in a region\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region : ndarray\n",
    "            Array of shape (N,) containing the values of the target values \n",
    "            for N datapoints in the training set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The sum of squared error\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        The error for an empty region should be infinity\n",
    "        This avoids creating empty regions\n",
    "        \"\"\"\n",
    "        N = region.shape[0]\n",
    "        if N > 0:\n",
    "            return np.sum(region - region.mean())**2\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        self.root_ = self.get_split(X, y)\n",
    "        self.recursive_growth(self.root_, 1, X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_sample(node, sample):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dictionary\n",
    "            A node created one of the methods above\n",
    "        sample : array of size (n_features,)\n",
    "            a sample datapoint\n",
    "        \"\"\"\n",
    "        if 'value' in node.keys():\n",
    "            return node['value']\n",
    "        if sample[node['feature_index']] < node['tau']:\n",
    "            return DTRegressor.predict_sample(node['left'], sample)\n",
    "        return DTRegressor.predict_sample(node['right'], sample)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of size (n_samples, n_features)\n",
    "            n_samples predictions will be made\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        X = check_array(X)\n",
    "\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i, sample in enumerate(X):\n",
    "            prediction[i] = self.predict_sample(self.root_, sample)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a008ef4-769c-4fd1-a0fb-e9735f891d47",
   "metadata": {},
   "source": [
    "We can test this implementation also using the code from the last lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ec92b-e7b5-44df-beca-ca19779410e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE : 4.582477460507463\n",
      "Test MSE : 4.704711300439497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dt = DTRegressor(min_samples=20, max_depth=6)\n",
    "dt.fit(X_train, y_train)\n",
    "test_mse = mean_squared_error(y_test, dt.predict(X_test))\n",
    "train_mse = mean_squared_error(y_train, dt.predict(X_train))\n",
    "\n",
    "print(f'Train MSE : {train_mse}')\n",
    "print(f'Test MSE : {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a72aad-8999-4d3c-9d93-ed1fe41f9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, num_bags=10):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    * The\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) # you can change the seed, or use 0 to replicate my results\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9c97a0-5781-4bc8-9e6a-832252c40e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 63, 51, 26, 30,  4,  7,  1, 17, 81, 64, 91, 50, 60, 97, 72, 63,\n",
       "       54, 55, 93, 27, 81, 67,  0, 39, 85, 55,  3, 76, 72, 84, 17,  8, 86,\n",
       "        2, 54,  8, 29, 48, 42, 40,  2,  0, 12,  0, 67, 52, 64, 25, 61, 76,\n",
       "       38, 46, 99, 80, 98, 37, 68, 95, 65, 84, 68, 70, 38, 87, 13, 57, 72,\n",
       "       84, 52, 37, 31, 42, 48, 71, 88,  7, 93, 53, 35, 67, 57, 25, 32, 71,\n",
       "       59, 50, 33, 76, 39, 32, 89, 26, 22, 71, 62,  4,  8, 37, 83])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877dd90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb3e484-fb1d-49c6-b042-60e7dd91ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133c3de",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "A convenient way to implement this, is to specialize the existing implementation of the `DTRegressor` class that is provided in this lab, add a couple of parameters, and overrride a small set of functions (namely, `fit`, `predict`, and `get_split`). If you don't feel comfortable using object oriented inheritance, you can always copy the code above, and make the necessary modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7fa984-dc1d-4967-a3d6-2e06dec2eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFRegressor(DTRegressor):\n",
    "    \"\"\"\n",
    "    A CART-based random forest for regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_samples : int\n",
    "        parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "    max_depth : int\n",
    "        parameter for stopping criterion if a node belongs to this depth\n",
    "    num_estimators : int, default 10\n",
    "        The number of trees in the forest.\n",
    "    num_features : int\n",
    "        The number of features to consider at each split\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    This implementation uses bootstrap samples, each estimator is trained\n",
    "    on a different sample of datapoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples, max_depth, num_estimators, num_features):\n",
    "        # Your code here\n",
    "\n",
    "    def get_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array (n_samples, n_features)\n",
    "            features \n",
    "        y : array (n_samples, )\n",
    "            labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decision : dictionary\n",
    "            keys are:\n",
    "            * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "            * 'tau' -> the threshold used to make the split\n",
    "            * 'low_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "            * 'high_region' -> indices not in `low_region`\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Your code here\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Your code here\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3ef7a-68ed-4ecd-9bad-8f9612ac83e4",
   "metadata": {},
   "source": [
    "The dataset we used for part 1 has very few features to be useful for random forest, for this, we can use scikit learn and create a random regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b2cc19-67a7-4bf0-92af-6f6c8d045ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE : 36032.860933058844\n",
      "Test MSE : 29970.33167009771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=400, n_features=15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "num_features = X_train.shape[1] // 3\n",
    "\n",
    "rf = RFRegressor(min_samples=20, max_depth=6, num_estimators=10, num_features=num_features)\n",
    "rf.fit(X_train, y_train)\n",
    "test_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "train_mse = mean_squared_error(y_train, rf.predict(X_train))\n",
    "\n",
    "print(f'Train MSE : {train_mse}')\n",
    "print(f'Test MSE : {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab52b74-6c0a-4908-82ed-16777994c211",
   "metadata": {},
   "source": [
    "These numbers look absolutely massive compared to the numbers we obtained in the previous lab. But remember that they don't mean much in isolation, but only when compared to another model trained on the same training set, and tested on the same testing set. \n",
    "\n",
    "Let's see the performance of `DTRegressor` on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f30c8bb-69f8-4223-9572-4a142895238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE : 34747.7357686727\n",
      "Test MSE : 28375.69828304471\n"
     ]
    }
   ],
   "source": [
    "dt = DTRegressor(min_samples=20, max_depth=6)\n",
    "dt.fit(X_train, y_train)\n",
    "test_mse = mean_squared_error(y_test, dt.predict(X_test))\n",
    "train_mse = mean_squared_error(y_train, dt.predict(X_train))\n",
    "\n",
    "print(f'Train MSE : {train_mse}')\n",
    "print(f'Test MSE : {test_mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
